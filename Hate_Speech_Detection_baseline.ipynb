{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hate_Speech_Detection_baseline.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"jincEkSv2-my","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1589376438085,"user_tz":-420,"elapsed":40527,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"2a5c6979-c498-4898-9325-6cdfa34216ea"},"source":["# Dataset analysis\n","import pandas as pd \n","from pyvi.ViTokenizer import ViTokenizer\n","\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","DATA_NEW_10k = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset_new_10k.csv'\n","data = pd.read_csv(DATA, index_col=False)\n","data_new = pd.read_csv(DATA_NEW_10k, index_col=False)\n","\n","data = pd.concat([data, data_new])\n","\n","data['label_id'] = data['label_id'].fillna(-1).astype(int)\n","\n","label0 = data.loc[data['label_id']==0]\n","label1 = data.loc[data['label_id']==1]\n","label2 = data.loc[data['label_id']==2]\n","\n","print(\"Total data:\", len(data))\n","print(\"Total data in label 0: \", len(label0))\n","print(\"Total data in label 1: \", len(label1))\n","print(\"Total data in label 2: \", len(label2))\n","\n","def get_total_words(dt):\n","    texts = dt['free_text']\n","    count = 0\n","    for t in texts:\n","        tokenized_t = ViTokenizer.tokenize(str(t))\n","        list_words = tokenized_t.split()\n","        count = count + len(list_words)\n","\n","    return count\n","\n","\n","print(\"Vocabulary size in labels 0:\", get_total_words(label0))\n","print(\"Vocabulary size in labels 1:\", get_total_words(label1))\n","print(\"Vocabulary size in labels 2:\", get_total_words(label2))\n","print(\"Total Vocabulary size:\", get_total_words(data))\n","\n","print(\"Average words in labels 0:\", get_total_words(label0)/len(label0))\n","print(\"Average words in labels 1:\", get_total_words(label1)/len(label1))\n","print(\"Average words in labels 2:\", get_total_words(label2)/len(label2))\n","print(\"Average words:\", get_total_words(data)/len(data))\n","\n","data.to_csv('drive/My Drive/CODE/Hate speech detection/data/hsd_data_new.csv', index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Total data: 31337\n","Total data in label 0:  27045\n","Total data in label 1:  2463\n","Total data in label 2:  1829\n","Vocabulary size in labels 0: 425002\n","Vocabulary size in labels 1: 28350\n","Vocabulary size in labels 2: 37025\n","Total Vocabulary size: 490377\n","Average words in labels 0: 15.7146237751895\n","Average words in labels 1: 11.510353227771011\n","Average words in labels 2: 20.243302351011483\n","Average words: 15.64849857995341\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tAn7AMBBxNNl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589508316818,"user_tz":-420,"elapsed":3068959,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"06a5f9c7-42e3-499b-f5ed-2633d81eea3b"},"source":["# GRU - Gate recurrent units\n","\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","# pre-process function\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","# Configuration - please change theses setting compatible with yours\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model_social/GRU_model_ccSC.h5'\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","# read data\n","data = pd.read_csv(DATA, index_col=False)\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","tokenizer = text.Tokenizer(num_words=None, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","\n","#--------------END TRICH XUAT DAC TRUNG -------------------------\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, embed_size))\n","max_features = num_words\n","\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# ------------------- XAY DUNG MO HINH MANG NEURAL -----------------------\n","inp = Input(shape=(maxlen,))\n","x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","x = SpatialDropout1D(0.2)(x)\n","x = Bidirectional(GRU(80, return_sequences=True))(x)\n","avg_pool = GlobalAveragePooling1D()(x)\n","max_pool = GlobalMaxPooling1D()(x)\n","conc = concatenate([avg_pool, max_pool])\n","outp = Dense(3, activation=\"sigmoid\")(conc)\n","\n","model = Model(inputs=inp, outputs=outp)\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","# ------------------- END XAY DUNG MO HINH MANG NEURAL -----------------------\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in list(X[train])]\n","    X_train_fold = tokenizer.texts_to_sequences(X_train_fold)\n","    X_train_fold = sequence.pad_sequences(X_train_fold, maxlen=maxlen)\n","\n","    X_test_fold = [preprocess(str(p)) for p in list(X[test])]\n","    X_test_fold = tokenizer.texts_to_sequences(X_test_fold)\n","    X_test_fold = sequence.pad_sequences(X_test_fold, maxlen=maxlen)\n","\n","    y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","    y_test_fold = y_test_fold\n","\n","    model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","    prediction = model.predict(X_test_fold, batch_size=batch_size, verbose=1)\n","    test_pred = prediction.argmax(axis=-1)\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        model.save('drive/My Drive/CODE/Hate speech detection/model_social/GRU/gru_model.h5')\n","        acc = evaluate\n","\n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","16276/16276 [==============================] - 64s 4ms/step - loss: 0.4332 - accuracy: 0.9412\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.2292 - accuracy: 0.9433\n","Epoch 3/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.2080 - accuracy: 0.9433\n","Epoch 4/10\n","16276/16276 [==============================] - 58s 4ms/step - loss: 0.1912 - accuracy: 0.9433\n","Epoch 5/10\n","16276/16276 [==============================] - 58s 4ms/step - loss: 0.1704 - accuracy: 0.9440\n","Epoch 6/10\n","16276/16276 [==============================] - 58s 4ms/step - loss: 0.1449 - accuracy: 0.9510\n","Epoch 7/10\n","16276/16276 [==============================] - 58s 4ms/step - loss: 0.1229 - accuracy: 0.9551\n","Epoch 8/10\n","16276/16276 [==============================] - 57s 4ms/step - loss: 0.1066 - accuracy: 0.9591\n","Epoch 9/10\n","16276/16276 [==============================] - 57s 4ms/step - loss: 0.0951 - accuracy: 0.9629\n","Epoch 10/10\n","16276/16276 [==============================] - 57s 4ms/step - loss: 0.0862 - accuracy: 0.9652\n","4069/4069 [==============================] - 1s 364us/step\n","===============================================\n","FOLD 1: 0.6535180730154556\n","[[3684   22   17]\n"," [ 121   62   21]\n"," [  56   13   73]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 58s 4ms/step - loss: 0.0969 - accuracy: 0.9641\n","Epoch 2/10\n","16276/16276 [==============================] - 57s 4ms/step - loss: 0.0863 - accuracy: 0.9678\n","Epoch 3/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0789 - accuracy: 0.9700\n","Epoch 4/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0732 - accuracy: 0.9728\n","Epoch 5/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0684 - accuracy: 0.9748\n","Epoch 6/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0646 - accuracy: 0.9761\n","Epoch 7/10\n","16276/16276 [==============================] - 57s 4ms/step - loss: 0.0613 - accuracy: 0.9777\n","Epoch 8/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0584 - accuracy: 0.9785\n","Epoch 9/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0557 - accuracy: 0.9806\n","Epoch 10/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0522 - accuracy: 0.9820\n","4069/4069 [==============================] - 1s 341us/step\n","===============================================\n","FOLD 2: 0.7064977830679461\n","[[3675   34   14]\n"," [  87   88   29]\n"," [  40   17   85]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0604 - accuracy: 0.9792\n","Epoch 2/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0540 - accuracy: 0.9813\n","Epoch 3/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0499 - accuracy: 0.9827\n","Epoch 4/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0459 - accuracy: 0.9845\n","Epoch 5/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0432 - accuracy: 0.9857\n","Epoch 6/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0405 - accuracy: 0.9869\n","Epoch 7/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0376 - accuracy: 0.9886\n","Epoch 8/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0351 - accuracy: 0.9894\n","Epoch 9/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0336 - accuracy: 0.9901\n","Epoch 10/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0320 - accuracy: 0.9905\n","4069/4069 [==============================] - 1s 333us/step\n","===============================================\n","FOLD 3: 0.7761957165200927\n","[[3690   24    9]\n"," [  76  107   21]\n"," [  23   18  101]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0409 - accuracy: 0.9869\n","Epoch 2/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0345 - accuracy: 0.9897\n","Epoch 3/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0318 - accuracy: 0.9906\n","Epoch 4/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0295 - accuracy: 0.9920\n","Epoch 5/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0273 - accuracy: 0.9923\n","Epoch 6/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0265 - accuracy: 0.9928\n","Epoch 7/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0252 - accuracy: 0.9930\n","Epoch 8/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0239 - accuracy: 0.9933\n","Epoch 9/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0231 - accuracy: 0.9938\n","Epoch 10/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0217 - accuracy: 0.9942\n","4069/4069 [==============================] - 1s 336us/step\n","===============================================\n","FOLD 4: 0.8431994987285479\n","[[3697   18    8]\n"," [  43  144   17]\n"," [  19   15  108]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0325 - accuracy: 0.9903\n","Epoch 2/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0293 - accuracy: 0.9914\n","Epoch 3/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0257 - accuracy: 0.9929\n","Epoch 4/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0235 - accuracy: 0.9937\n","Epoch 5/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0223 - accuracy: 0.9941\n","Epoch 6/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0216 - accuracy: 0.9944\n","Epoch 7/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0207 - accuracy: 0.9944\n","Epoch 8/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0198 - accuracy: 0.9944\n","Epoch 9/10\n","16276/16276 [==============================] - 57s 3ms/step - loss: 0.0192 - accuracy: 0.9949\n","Epoch 10/10\n","16276/16276 [==============================] - 56s 3ms/step - loss: 0.0188 - accuracy: 0.9949\n","4069/4069 [==============================] - 1s 348us/step\n","===============================================\n","FOLD 5: 0.8780400718404611\n","[[3705   15    3]\n"," [  37  153   14]\n"," [   9   15  118]]\n","===============================================\n","average acc: 0.7714902286345007\n","average conf mat: [[3690.2   22.6   10.2]\n"," [  72.8  110.8   20.4]\n"," [  29.4   15.6   97. ]]\n","Best accuracy: 0.8780400718404611\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PYYOusO8mpYt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1590919648376,"user_tz":-420,"elapsed":626108,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"d6eab2d7-cf68-4783-c1d4-f9eb4b57d09b"},"source":["# Text CNN \n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","# pre-process function\n","def preprocess(text):\n","    text = text.split(\" \")\n","    text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","# configuration \n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model/TextCNN_model_ccSC_new10k.h5'\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 20987\n","maxlen = 1000\n","\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","# read data\n","data = pd.read_csv(DATA)\n","print(len(data))\n","\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","# tokenizer = text.Tokenizer(num_words=None, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n","tokenizer = text.Tokenizer(num_words=None, lower=True)\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","\n","# --------------END TRICH XUAT DAC TRUNG -------------------------\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, embed_size))\n","\n","max_features = num_words\n","\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","\n","# ------------------- XAY DUNG MO HINH MANG NEURAL -----------------------\n","filter_sizes = [1,2,3,5]\n","num_filters = 32\n","\n","inp = Input(shape=(maxlen,))\n","x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","x = SpatialDropout1D(0.4)(x)\n","x = Reshape((maxlen, embed_size, 1))(x)\n","\n","conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","\n","maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n","maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n","maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n","maxpool_3 = MaxPool2D(pool_size=(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n","\n","z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n","z = Flatten()(z)\n","z = Dropout(0.1)(z)\n","\n","\n","outp = Dense(3, activation=\"sigmoid\")(z)\n","\n","model = Model(inputs=inp, outputs=outp)\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","# ------------------- END XAY DUNG MO HINH MANG NEURAL -----------------------\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in list(X[train])]\n","    X_train_fold = tokenizer.texts_to_sequences(X_train_fold)\n","    X_train_fold = sequence.pad_sequences(X_train_fold, maxlen=maxlen)\n","\n","    X_test_fold = [preprocess(str(p)) for p in list(X[test])]\n","    X_test_fold = tokenizer.texts_to_sequences(X_test_fold)\n","    X_test_fold = sequence.pad_sequences(X_test_fold, maxlen=maxlen)\n","\n","    y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","    y_test_fold = y_test_fold\n","\n","    model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","    prediction = model.predict(X_test_fold, batch_size=batch_size, verbose=1)\n","    test_pred = prediction.argmax(axis=-1)\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        model.save('drive/My Drive/CODE/Hate speech detection/model_social/TextCNN/textcnn_model_new_10k.h5')\n","        acc = evaluate\n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["20345\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","16276/16276 [==============================] - 11s 673us/step - loss: 0.3542 - accuracy: 0.8845\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 556us/step - loss: 0.2137 - accuracy: 0.9433\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.1829 - accuracy: 0.9444\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 555us/step - loss: 0.1555 - accuracy: 0.9492\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.1335 - accuracy: 0.9549\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.1150 - accuracy: 0.9602\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.1028 - accuracy: 0.9629\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 553us/step - loss: 0.0947 - accuracy: 0.9643\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0868 - accuracy: 0.9670\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0816 - accuracy: 0.9689\n","4069/4069 [==============================] - 2s 380us/step\n","===============================================\n","FOLD 1: 0.6339495489825886\n","[[3706   12    5]\n"," [ 125   59   20]\n"," [  62   20   60]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.0848 - accuracy: 0.9683\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0781 - accuracy: 0.9711\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.0737 - accuracy: 0.9731\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.0678 - accuracy: 0.9744\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0629 - accuracy: 0.9761\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 553us/step - loss: 0.0593 - accuracy: 0.9774\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0558 - accuracy: 0.9790\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0521 - accuracy: 0.9801\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0470 - accuracy: 0.9826\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0450 - accuracy: 0.9840\n","4069/4069 [==============================] - 1s 187us/step\n","===============================================\n","FOLD 2: 0.7238596985331908\n","[[3686   21   16]\n"," [  87   97   20]\n"," [  46   16   80]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0529 - accuracy: 0.9805\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0482 - accuracy: 0.9825\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 556us/step - loss: 0.0434 - accuracy: 0.9843\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 553us/step - loss: 0.0400 - accuracy: 0.9857\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 553us/step - loss: 0.0354 - accuracy: 0.9879\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 554us/step - loss: 0.0339 - accuracy: 0.9878\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.0313 - accuracy: 0.9892\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0283 - accuracy: 0.9903\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0269 - accuracy: 0.9908\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0251 - accuracy: 0.9916\n","4069/4069 [==============================] - 1s 187us/step\n","===============================================\n","FOLD 3: 0.8425979096643826\n","[[3708   11    4]\n"," [  57  135   12]\n"," [  30    9  103]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0309 - accuracy: 0.9894\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0273 - accuracy: 0.9908\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0241 - accuracy: 0.9925\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0218 - accuracy: 0.9929\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0201 - accuracy: 0.9936\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 554us/step - loss: 0.0185 - accuracy: 0.9944\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0175 - accuracy: 0.9949\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 555us/step - loss: 0.0160 - accuracy: 0.9954\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0154 - accuracy: 0.9954\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0150 - accuracy: 0.9956\n","4069/4069 [==============================] - 1s 187us/step\n","===============================================\n","FOLD 4: 0.904294585995897\n","[[3704   13    6]\n"," [  41  157    6]\n"," [  14    4  124]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0202 - accuracy: 0.9934\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0180 - accuracy: 0.9943\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0159 - accuracy: 0.9947\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 557us/step - loss: 0.0144 - accuracy: 0.9955\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 559us/step - loss: 0.0129 - accuracy: 0.9961\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 555us/step - loss: 0.0127 - accuracy: 0.9961\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 554us/step - loss: 0.0116 - accuracy: 0.9966\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 558us/step - loss: 0.0108 - accuracy: 0.9967\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 553us/step - loss: 0.0107 - accuracy: 0.9969\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 555us/step - loss: 0.0103 - accuracy: 0.9969\n","4069/4069 [==============================] - 1s 187us/step\n","===============================================\n","FOLD 5: 0.9450676018053382\n","[[3711   11    1]\n"," [  25  177    2]\n"," [   6    5  131]]\n","===============================================\n","average acc: 0.8099538689962793\n","average conf mat: [[3703.    13.6    6.4]\n"," [  67.   125.    12. ]\n"," [  31.6   10.8   99.6]]\n","Best accuracy: 0.9450676018053382\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gp-TM7X1s1m_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1589512113476,"user_tz":-420,"elapsed":2655144,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"6de3276b-7a4a-4692-8f9a-51af3e2957d6"},"source":["# Bi LSTM \n","\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, \\\n","    GlobalMaxPool1D, LSTM\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","# Pre-process function\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model/BiLSTM_model_ccSC.h5'\n","\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","# read data\n","data = pd.read_csv(DATA)\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","# Vectorize text + Prepare GloVe Embedding\n","tokenizer = text.Tokenizer(num_words=None, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","\n","# --------------END TRICH XUAT DAC TRUNG -------------------------\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, embed_size))\n","max_features = num_words\n","\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# ------------------- XAY DUNG MO HINH MANG NEURAL -----------------------\n","inp = Input(shape=(maxlen, ))\n","x = Embedding(max_features, embed_size)(inp)\n","x = Bidirectional(LSTM(50, return_sequences=True))(x)\n","x = GlobalMaxPool1D()(x)\n","x = Dropout(0.1)(x)\n","x = Dense(50, activation=\"relu\")(x)\n","x = Dropout(0.1)(x)\n","x = Dense(3, activation=\"sigmoid\")(x)\n","model = Model(inputs=inp, outputs=x)\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","# ------------------- END XAY DUNG MO HINH MANG NEURAL -----------------------\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in list(X[train])]\n","    X_train_fold = tokenizer.texts_to_sequences(X_train_fold)\n","    X_train_fold = sequence.pad_sequences(X_train_fold, maxlen=maxlen)\n","\n","    X_test_fold = [preprocess(str(p)) for p in list(X[test])]\n","    X_test_fold = tokenizer.texts_to_sequences(X_test_fold)\n","    X_test_fold = sequence.pad_sequences(X_test_fold, maxlen=maxlen)\n","\n","    y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","    y_test_fold = y_test_fold\n","\n","    model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","    prediction = model.predict(X_test_fold, batch_size=batch_size, verbose=1)\n","    test_pred = prediction.argmax(axis=-1)\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        model.save('drive/My Drive/CODE/Hate speech detection/model_social/BiLSTM/bilstm_model.h5')\n","        acc = evaluate\n","    \n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.4441 - accuracy: 0.8903\n","Epoch 2/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.3026 - accuracy: 0.9087\n","Epoch 3/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.2848 - accuracy: 0.9087\n","Epoch 4/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.2213 - accuracy: 0.9153\n","Epoch 5/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1797 - accuracy: 0.9287\n","Epoch 6/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1597 - accuracy: 0.9342\n","Epoch 7/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1466 - accuracy: 0.9386\n","Epoch 8/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1357 - accuracy: 0.9433\n","Epoch 9/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1280 - accuracy: 0.9471\n","Epoch 10/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.1186 - accuracy: 0.9526\n","6268/6268 [==============================] - 2s 320us/step\n","===============================================\n","FOLD 1: 0.5884989887249847\n","[[5141  179   89]\n"," [ 269  161   63]\n"," [ 113   91  162]]\n","===============================================\n","Epoch 1/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.1382 - accuracy: 0.9491\n","Epoch 2/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1228 - accuracy: 0.9563\n","Epoch 3/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.1125 - accuracy: 0.9602\n","Epoch 4/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.1109 - accuracy: 0.9608\n","Epoch 5/10\n","25069/25069 [==============================] - 73s 3ms/step - loss: 0.0988 - accuracy: 0.9661\n","Epoch 6/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0905 - accuracy: 0.9702\n","Epoch 7/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0836 - accuracy: 0.9720\n","Epoch 8/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0786 - accuracy: 0.9742\n","Epoch 9/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0733 - accuracy: 0.9761\n","Epoch 10/10\n","25069/25069 [==============================] - 71s 3ms/step - loss: 0.0686 - accuracy: 0.9775\n","6268/6268 [==============================] - 2s 305us/step\n","===============================================\n","FOLD 2: 0.6731338404908982\n","[[5219  122   68]\n"," [ 192  209   92]\n"," [  96   52  218]]\n","===============================================\n","Epoch 1/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0969 - accuracy: 0.9688\n","Epoch 2/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0823 - accuracy: 0.9728\n","Epoch 3/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0734 - accuracy: 0.9763\n","Epoch 4/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0675 - accuracy: 0.9781\n","Epoch 5/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0625 - accuracy: 0.9799\n","Epoch 6/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0586 - accuracy: 0.9813\n","Epoch 7/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0561 - accuracy: 0.9819\n","Epoch 8/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0530 - accuracy: 0.9826\n","Epoch 9/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0515 - accuracy: 0.9830\n","Epoch 10/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0499 - accuracy: 0.9838\n","6268/6268 [==============================] - 2s 302us/step\n","===============================================\n","FOLD 3: 0.7690390288716608\n","[[5286   88   35]\n"," [ 152  297   44]\n"," [  82   46  238]]\n","===============================================\n","Epoch 1/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0711 - accuracy: 0.9779\n","Epoch 2/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0605 - accuracy: 0.9805\n","Epoch 3/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0559 - accuracy: 0.9823\n","Epoch 4/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0504 - accuracy: 0.9837\n","Epoch 5/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0477 - accuracy: 0.9848\n","Epoch 6/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0454 - accuracy: 0.9858\n","Epoch 7/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0435 - accuracy: 0.9859\n","Epoch 8/10\n","25069/25069 [==============================] - 71s 3ms/step - loss: 0.0417 - accuracy: 0.9866\n","Epoch 9/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0414 - accuracy: 0.9868\n","Epoch 10/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0407 - accuracy: 0.9870\n","6268/6268 [==============================] - 2s 306us/step\n","===============================================\n","FOLD 4: 0.825934514607532\n","[[5297   80   32]\n"," [ 106  349   38]\n"," [  45   43  278]]\n","===============================================\n","Epoch 1/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0581 - accuracy: 0.9812\n","Epoch 2/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0484 - accuracy: 0.9842\n","Epoch 3/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0435 - accuracy: 0.9860\n","Epoch 4/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0403 - accuracy: 0.9873\n","Epoch 5/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0412 - accuracy: 0.9871\n","Epoch 6/10\n","25069/25069 [==============================] - 71s 3ms/step - loss: 0.0400 - accuracy: 0.9873\n","Epoch 7/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0381 - accuracy: 0.9880\n","Epoch 8/10\n","25069/25069 [==============================] - 71s 3ms/step - loss: 0.0347 - accuracy: 0.9892\n","Epoch 9/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0352 - accuracy: 0.9892\n","Epoch 10/10\n","25069/25069 [==============================] - 72s 3ms/step - loss: 0.0347 - accuracy: 0.9888\n","6268/6268 [==============================] - 2s 301us/step\n","===============================================\n","FOLD 5: 0.8572682289639184\n","[[5342   41   26]\n"," [ 115  347   31]\n"," [  46   23  297]]\n","===============================================\n","average acc: 0.7427749203317988\n","average conf mat: [[5257.   102.    50. ]\n"," [ 166.8  272.6   53.6]\n"," [  76.4   51.   238.6]]\n","Best accuracy: 0.8572682289639184\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6EDIqkyVh6CJ","colab_type":"code","colab":{}},"source":["# LSTM \n","\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout, \\\n","    GlobalMaxPool1D, LSTM\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Activation\n","from keras.layers.normalization import BatchNormalization\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","\n","from sklearn.metrics import f1_score\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","# support function\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","class Attention(Layer):\n","    def __init__(self, step_dim,\n","                 W_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","        \"\"\"\n","        Keras Layer that implements an Attention mechanism for temporal data.\n","        Supports Masking.\n","        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n","        # Input shape\n","            3D tensor with shape: `(samples, steps, features)`.\n","        # Output shape\n","            2D tensor with shape: `(samples, features)`.\n","        :param kwargs:\n","        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","        The dimensions are inferred based on the output shape of the RNN.\n","        Example:\n","            model.add(LSTM(64, return_sequences=True))\n","            model.add(Attention())\n","        \"\"\"\n","        self.supports_masking = True\n","        #self.init = initializations.get('glorot_uniform')\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        self.features_dim = input_shape[-1]\n","\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","        else:\n","            self.b = None\n","\n","        self.built = True\n","\n","    def compute_mask(self, input, input_mask=None):\n","        # do not pass the mask to the next layers\n","        return None\n","\n","    def call(self, x, mask=None):\n","        # eij = K.dot(x, self.W) TF backend doesn't support it\n","\n","        # features_dim = self.W.shape[0]\n","        # step_dim = x._keras_shape[1]\n","\n","        features_dim = self.features_dim\n","        step_dim = self.step_dim\n","\n","        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n","\n","        if self.bias:\n","            eij += self.b\n","\n","        eij = K.tanh(eij)\n","\n","        a = K.exp(eij)\n","\n","        # apply mask after the exp. will be re-normalized next\n","        if mask is not None:\n","            # Cast the mask to floatX to avoid float64 upcasting in theano\n","            a *= K.cast(mask, K.floatx())\n","\n","        # in some cases especially in the early stages of training the sum may be almost zero\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","    #print weigthted_input.shape\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        #return input_shape[0], input_shape[-1]\n","        return input_shape[0],  self.features_dim\n","\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model/LSTM_model_ccSC.h5'\n","\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/hsd_data_new.csv'\n","# DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","num_lstm = 300\n","num_dense = 256\n","rate_drop_lstm = 0.25\n","rate_drop_dense = 0.25\n","\n","act = 'relu'\n","\n","# read data\n","data = pd.read_csv(DATA)\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","# Vectorize text + Prepare GloVe Embedding\n","tokenizer = text.Tokenizer(num_words=None, lower=True, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^`{|}~\\t\\n')\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","\n","# --------------END TRICH XUAT DAC TRUNG -------------------------\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = len(word_index) + 1\n","embedding_matrix = np.zeros((num_words, embed_size))\n","max_features = num_words\n","\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# ------------------- XAY DUNG MO HINH MANG NEURAL -----------------------\n","inp = Input(shape=(maxlen, ))\n","embedded_sequences = Embedding(max_features, embed_size)(inp)\n","lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm,return_sequences=True)\n","x = lstm_layer(embedded_sequences)\n","x = Dropout(rate_drop_dense)(x)\n","merged = Attention(maxlen)(x)\n","merged = Dense(num_dense, activation=act)(merged)\n","merged = Dropout(rate_drop_dense)(merged)\n","merged = BatchNormalization()(merged)\n","preds = Dense(6, activation='sigmoid')(merged)\n","\n","model = Model(inputs=inp, outputs=preds)\n","model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","# ------------------- END XAY DUNG MO HINH MANG NEURAL -----------------------\n","\n","# Kfold\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in list(X[train])]\n","    X_train_fold = tokenizer.texts_to_sequences(X_train_fold)\n","    X_train_fold = sequence.pad_sequences(X_train_fold, maxlen=maxlen)\n","\n","    X_test_fold = [preprocess(str(p)) for p in list(X[test])]\n","    X_test_fold = tokenizer.texts_to_sequences(X_test_fold)\n","    X_test_fold = sequence.pad_sequences(X_test_fold, maxlen=maxlen)\n","\n","    y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","    y_test_fold = y_test_fold\n","\n","    model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","    prediction = model.predict(X_test_fold, batch_size=batch_size, verbose=1)\n","    test_pred = prediction.argmax(axis=-1)\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        model.save('drive/My Drive/CODE/Hate speech detection/model_social/LSTM/bilstm_model.h5')\n","        acc = evaluate\n","    \n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a4d7z38JWpN5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":871},"executionInfo":{"status":"ok","timestamp":1580099488743,"user_tz":-420,"elapsed":568801,"user":{"displayName":"Son Luu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOpA0EIHM56-K2ejgUcaTAXf9iBw3YjudaVluj=s64","userId":"07816337279734159476"}},"outputId":"86398d0a-7078-4b21-9a7b-78a1d4dbd2ee"},"source":["# SVM\n","import pandas as pd\n","\n","# read data\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.pipeline import FeatureUnion\n","import numpy as np\n","from unidecode import unidecode\n","from joblib import dump\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.preprocessing import FunctionTransformer\n","\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","BADWORDS = 'drive/My Drive/CODE/Hate speech detection/bad_words.txt'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/MODEL/SVM.joblib'\n","STOPWORDS = 'drive/My Drive/CODE/Hate speech detection/stopwords.txt'\n","\n","original_data = pd.read_csv('drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv')\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","# O_X = [preprocess(str(t)) for t in O_X]\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        stop_words.append(dd)\n","\n","\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","bad_words_count = TfidfVectorizer(vocabulary=set(badwords), lowercase=True)\n","\n","features_extractor = FeatureUnion([                              \n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count), \n","])\n","\n","# Train and test division\n","train_set = O_X\n","target_set = O_y\n","\n","features_extractor.fit([preprocess(str(t)) for t in train_set])\n","\n","# Build Model\n","model = svm.SVC(kernel='linear', C=1)\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(t)) for t in X_train_fold]\n","    X_train_fold = features_extractor.transform(X_train_fold)\n","\n","    X_test_fold = [preprocess(str(t)) for t in X_test_fold]\n","    X_test_fold = features_extractor.transform(X_test_fold)\n","\n","    model.fit(X_train_fold, y_train_fold)\n","    prediction = model.predict(X_test_fold)\n","\n","    test_pred = prediction\n","\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        dump(model, 'drive/My Drive/CODE/Hate speech detection/model_social/SVM/svm_model.h5')\n","        acc = evaluate\n","        \n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(confuses)\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 1: 0.6572873941816839\n","[[3703   10   10]\n"," [ 121   68   15]\n"," [  59   20   63]]\n","===============================================\n","===============================================\n","FOLD 2: 0.6580362774100347\n","[[3683   24   16]\n"," [ 119   65   20]\n"," [  47   21   74]]\n","===============================================\n","===============================================\n","FOLD 3: 0.647673545712958\n","[[3688   21   14]\n"," [ 121   66   17]\n"," [  45   30   67]]\n","===============================================\n","===============================================\n","FOLD 4: 0.6722246266202124\n","[[3678   27   18]\n"," [ 118   70   16]\n"," [  50   16   76]]\n","===============================================\n","===============================================\n","FOLD 5: 0.6866230291881834\n","[[3681   24   18]\n"," [ 111   72   21]\n"," [  44   15   83]]\n","===============================================\n","average acc: 0.6643689746226145\n","[array([[3703,   10,   10],\n","       [ 121,   68,   15],\n","       [  59,   20,   63]]), array([[3683,   24,   16],\n","       [ 119,   65,   20],\n","       [  47,   21,   74]]), array([[3688,   21,   14],\n","       [ 121,   66,   17],\n","       [  45,   30,   67]]), array([[3678,   27,   18],\n","       [ 118,   70,   16],\n","       [  50,   16,   76]]), array([[3681,   24,   18],\n","       [ 111,   72,   21],\n","       [  44,   15,   83]])]\n","average conf mat: [[3686.6   21.2   15.2]\n"," [ 118.    68.2   17.8]\n"," [  49.    20.4   72.6]]\n","Best accuracy: 0.6866230291881834\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"H9kFggZgjGxY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1580100319843,"user_tz":-420,"elapsed":156842,"user":{"displayName":"Son Luu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOpA0EIHM56-K2ejgUcaTAXf9iBw3YjudaVluj=s64","userId":"07816337279734159476"}},"outputId":"7a4a2779-8539-4f5d-e6a9-c1f29e1ab7b7"},"source":["# Logistic \n","\n","import pandas as pd\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.pipeline import FeatureUnion\n","import numpy as np\n","from unidecode import unidecode\n","from joblib import dump\n","\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/MODEL/Logistic.joblib'\n","STOPWORDS = 'drive/My Drive/CODE/Hate speech detection/stopwords.txt'\n","original_data = pd.read_csv('drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv')\n","BADWORDS = 'drive/My Drive/CODE/Hate speech detection/bad_words.txt'\n","\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        stop_words.append(line.strip('\\n'))\n","\n","\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","bad_words_count = TfidfVectorizer(vocabulary=set(badwords), lowercase=True)\n","\n","features_extractor = FeatureUnion([\n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count)\n","])\n","\n","# Train and test division\n","train_set = O_X\n","target_set = O_y\n","\n","features_extractor.fit([preprocess(str(t)) for t in train_set])\n","\n","# Build Model\n","model = LogisticRegression(C=2, class_weight='balanced')\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in X_train_fold]\n","    X_train_fold = features_extractor.transform(X_train_fold)\n","    \n","    X_test_fold = [preprocess(str(p)) for p in X_test_fold]\n","    X_test_fold = features_extractor.transform(X_test_fold)\n","    \n","\n","    model.fit(X_train_fold, y_train_fold)\n","    prediction = model.predict(X_test_fold)\n","\n","    test_pred = prediction\n","    \n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        dump(model, 'drive/My Drive/CODE/Hate speech detection/model_social/Logistic/logistic_model.h5')\n","        acc = evaluate\n","\n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(confuses)\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'stop_words' will not be used\"\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 1: 0.6351474258640382\n","[[3535  138   50]\n"," [  82   91   31]\n"," [  28   29   85]]\n","===============================================\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 2: 0.6765822217757322\n","[[3556  118   49]\n"," [  86   94   24]\n"," [  23   19  100]]\n","===============================================\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 3: 0.6418723416760023\n","[[3543  131   49]\n"," [  84  100   20]\n"," [  29   35   78]]\n","===============================================\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 4: 0.6542610247540747\n","[[3550  128   45]\n"," [  76   95   33]\n"," [  26   26   90]]\n","===============================================\n","===============================================\n","FOLD 5: 0.6685020794405822\n","[[3577   93   53]\n"," [  86   87   31]\n"," [  23   20   99]]\n","===============================================\n","average acc: 0.655273018702086\n","[array([[3535,  138,   50],\n","       [  82,   91,   31],\n","       [  28,   29,   85]]), array([[3556,  118,   49],\n","       [  86,   94,   24],\n","       [  23,   19,  100]]), array([[3543,  131,   49],\n","       [  84,  100,   20],\n","       [  29,   35,   78]]), array([[3550,  128,   45],\n","       [  76,   95,   33],\n","       [  26,   26,   90]]), array([[3577,   93,   53],\n","       [  86,   87,   31],\n","       [  23,   20,   99]])]\n","average conf mat: [[3552.2  121.6   49.2]\n"," [  82.8   93.4   27.8]\n"," [  25.8   25.8   90.4]]\n","Best accuracy: 0.6765822217757322\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"voWhRLIfbQKc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":871},"executionInfo":{"status":"ok","timestamp":1580974760919,"user_tz":-420,"elapsed":93576,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDbhay-xCL4lgV8CWywjs2lLHdBez_RFM1dhC9s=s64","userId":"09824077883060402796"}},"outputId":"e6f9833a-f5a0-47e1-c3bf-21d56aea06b2"},"source":["# Naive Bayes\n","\n","import pandas as pd\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.metrics import f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.pipeline import FeatureUnion\n","import numpy as np\n","from unidecode import unidecode\n","from joblib import dump\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/MODEL/Logistic.joblib'\n","STOPWORDS = 'drive/My Drive/CODE/Hate speech detection/stopwords.txt'\n","BADWORDS = 'drive/My Drive/CODE/Hate speech detection/bad_words.txt'\n","\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","original_data = pd.read_csv('drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv')\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        stop_words.append(line.strip('\\n'))\n","\n","\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","bad_words_count = TfidfVectorizer(vocabulary=(set(badwords)), lowercase=True)\n","\n","features_extractor = FeatureUnion([\n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count)\n","])\n","\n","# Train and test division\n","train_set = O_X\n","target_set = O_y\n","\n","\n","features_extractor.fit([preprocess(str(t)) for t in train_set])\n","\n","\n","# Build Model\n","model = MultinomialNB(alpha=0.1)\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in X_train_fold]\n","    X_train_fold = features_extractor.transform(X_train_fold)\n","    \n","    X_test_fold = [preprocess(str(p)) for p in X_test_fold]\n","    X_test_fold = features_extractor.transform(X_test_fold)\n","    \n","\n","    model.fit(X_train_fold, y_train_fold)\n","    prediction = model.predict(X_test_fold)\n","\n","    test_pred = prediction\n","    \n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        dump(model, 'drive/My Drive/CODE/Hate speech detection/model_social/Naive Bayes/naive_bayes_model.h5')\n","        acc = evaluate\n","\n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(confuses)\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n","print(\"Best accuracy: {}\".format(acc))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"],"name":"stderr"},{"output_type":"stream","text":["===============================================\n","FOLD 1: 0.6228704416847504\n","[[3476  154   93]\n"," [  66   93   45]\n"," [  26   18   98]]\n","===============================================\n","===============================================\n","FOLD 2: 0.6371886559944698\n","[[3460  171   92]\n"," [  66  101   37]\n"," [  21   18  103]]\n","===============================================\n","===============================================\n","FOLD 3: 0.6054809730792691\n","[[3437  188   98]\n"," [  71   89   44]\n"," [  25   19   98]]\n","===============================================\n","===============================================\n","FOLD 4: 0.6255473053590049\n","[[3438  172  113]\n"," [  59  107   38]\n"," [  25   19   98]]\n","===============================================\n","===============================================\n","FOLD 5: 0.5858524487501354\n","[[3397  206  120]\n"," [  74   84   46]\n"," [  20   22  100]]\n","===============================================\n","average acc: 0.6153879649735259\n","[array([[3476,  154,   93],\n","       [  66,   93,   45],\n","       [  26,   18,   98]]), array([[3460,  171,   92],\n","       [  66,  101,   37],\n","       [  21,   18,  103]]), array([[3437,  188,   98],\n","       [  71,   89,   44],\n","       [  25,   19,   98]]), array([[3438,  172,  113],\n","       [  59,  107,   38],\n","       [  25,   19,   98]]), array([[3397,  206,  120],\n","       [  74,   84,   46],\n","       [  20,   22,  100]])]\n","average conf mat: [[3441.6  178.2  103.2]\n"," [  67.2   94.8   42. ]\n"," [  23.4   19.2   99.4]]\n","Best accuracy: 0.6371886559944698\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"06fjUlR2lrPy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1580979324399,"user_tz":-420,"elapsed":3218924,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDbhay-xCL4lgV8CWywjs2lLHdBez_RFM1dhC9s=s64","userId":"09824077883060402796"}},"outputId":"7515d203-4550-4040-f25b-0e31df14f21f"},"source":["# BiLSTM + CNN\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n","from keras.callbacks import Callback\n","from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","# pre-process function\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/BiLST_TextCNN_model_ccSC.h5'\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","# read data\n","data = pd.read_csv(DATA)\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","\n","# --------------END TRICH XUAT DAC TRUNG -------------------------\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = min(max_features, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embed_size))\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","# ------------------- XAY DUNG MO HINH MANG NEURAL -----------------------\n","sequence_input = Input(shape=(maxlen, ))\n","x = Embedding(max_features, embed_size, weights=[embedding_matrix],trainable = False)(sequence_input)\n","x = SpatialDropout1D(0.2)(x)\n","x = Bidirectional(GRU(128, return_sequences=True,dropout=0.1,recurrent_dropout=0.1))(x)\n","x = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"glorot_uniform\")(x)\n","avg_pool = GlobalAveragePooling1D()(x)\n","max_pool = GlobalMaxPooling1D()(x)\n","x = concatenate([avg_pool, max_pool]) \n","preds = Dense(3, activation=\"sigmoid\")(x)\n","model = Model(sequence_input, preds)\n","model.compile(loss='binary_crossentropy',optimizer=Adam(lr=1e-3),metrics=['accuracy'])\n","# ------------------- END XAY DUNG MO HINH MANG NEURAL -----------------------\n","\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","acc = 0\n","\n","for train, test in kfold.split(X, y):\n","    X_train_fold = X[train]\n","    y_train_fold = y[train]\n","\n","    X_test_fold = X[test]\n","    y_test_fold = y[test]\n","\n","    X_train_fold = [preprocess(str(p)) for p in list(X[train])]\n","    X_train_fold = tokenizer.texts_to_sequences(X_train_fold)\n","    X_train_fold = sequence.pad_sequences(X_train_fold, maxlen=maxlen)\n","\n","    X_test_fold = [preprocess(str(p)) for p in list(X[test])]\n","    X_test_fold = tokenizer.texts_to_sequences(X_test_fold)\n","    X_test_fold = sequence.pad_sequences(X_test_fold, maxlen=maxlen)\n","\n","    y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","    y_test_fold = y_test_fold\n","\n","    model.fit(X_train_fold, y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","    prediction = model.predict(X_test_fold, batch_size=batch_size, verbose=1)\n","    test_pred = prediction.argmax(axis=-1)\n","    evaluate = f1_score(y_test_fold, test_pred, average='macro')\n","    confuse = confusion_matrix(y_test_fold, test_pred, labels=[0, 1, 2])\n","\n","    print('===============================================')\n","    print(\"FOLD {}: {}\".format(count, evaluate))\n","    print(confuse)\n","    results.append(evaluate)\n","    confuses.append(confuse)\n","    print('===============================================')\n","\n","    if evaluate > acc:\n","        model.save('drive/My Drive/CODE/Hate speech detection/model_social/BiLSTM-CNN/bilstm_cnn_model.h5')\n","        acc = evaluate\n","    count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/10\n","16276/16276 [==============================] - 76s 5ms/step - loss: 0.3766 - acc: 0.9041\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.2196 - acc: 0.9433\n","Epoch 3/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1926 - acc: 0.9434\n","Epoch 4/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1533 - acc: 0.9483\n","Epoch 5/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1342 - acc: 0.9534\n","Epoch 6/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1241 - acc: 0.9571\n","Epoch 7/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1208 - acc: 0.9580\n","Epoch 8/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1172 - acc: 0.9592\n","Epoch 9/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1162 - acc: 0.9596\n","Epoch 10/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1136 - acc: 0.9605\n","4069/4069 [==============================] - 4s 1ms/step\n","===============================================\n","FOLD 1: 0.6208266103807035\n","[[3714    5    4]\n"," [ 134   50   20]\n"," [  75    9   58]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1123 - acc: 0.9607\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1105 - acc: 0.9613\n","Epoch 3/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1085 - acc: 0.9620\n","Epoch 4/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1078 - acc: 0.9624\n","Epoch 5/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1060 - acc: 0.9624\n","Epoch 6/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.1027 - acc: 0.9634\n","Epoch 7/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1031 - acc: 0.9628\n","Epoch 8/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.1013 - acc: 0.9639\n","Epoch 9/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0997 - acc: 0.9639\n","Epoch 10/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0975 - acc: 0.9645\n","4069/4069 [==============================] - 4s 1000us/step\n","===============================================\n","FOLD 2: 0.6847656340632519\n","[[3671   28   24]\n"," [ 108   72   24]\n"," [  36   18   88]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0995 - acc: 0.9640\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0990 - acc: 0.9647\n","Epoch 3/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0944 - acc: 0.9662\n","Epoch 4/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0939 - acc: 0.9658\n","Epoch 5/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0926 - acc: 0.9672\n","Epoch 6/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0925 - acc: 0.9666\n","Epoch 7/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0895 - acc: 0.9675\n","Epoch 8/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0889 - acc: 0.9674\n","Epoch 9/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0874 - acc: 0.9688\n","Epoch 10/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0869 - acc: 0.9687\n","4069/4069 [==============================] - 4s 1ms/step\n","===============================================\n","FOLD 3: 0.7071129362416483\n","[[3683   34    6]\n"," [ 109   86    9]\n"," [  42   24   76]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0906 - acc: 0.9673\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0879 - acc: 0.9677\n","Epoch 3/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0872 - acc: 0.9687\n","Epoch 4/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0841 - acc: 0.9701\n","Epoch 5/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0840 - acc: 0.9701\n","Epoch 6/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0830 - acc: 0.9698\n","Epoch 7/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0828 - acc: 0.9703\n","Epoch 8/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0781 - acc: 0.9720\n","Epoch 9/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0800 - acc: 0.9706\n","Epoch 10/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0764 - acc: 0.9719\n","4069/4069 [==============================] - 4s 995us/step\n","===============================================\n","FOLD 4: 0.7437169312169312\n","[[3693   12   18]\n"," [  98   94   12]\n"," [  46   10   86]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0791 - acc: 0.9718\n","Epoch 2/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0768 - acc: 0.9724\n","Epoch 3/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0734 - acc: 0.9732\n","Epoch 4/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0727 - acc: 0.9734\n","Epoch 5/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0699 - acc: 0.9743\n","Epoch 6/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0683 - acc: 0.9744\n","Epoch 7/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0674 - acc: 0.9749\n","Epoch 8/10\n","16276/16276 [==============================] - 59s 4ms/step - loss: 0.0665 - acc: 0.9759\n","Epoch 9/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0634 - acc: 0.9765\n","Epoch 10/10\n","16276/16276 [==============================] - 60s 4ms/step - loss: 0.0607 - acc: 0.9774\n","4069/4069 [==============================] - 4s 1ms/step\n","===============================================\n","FOLD 5: 0.7657016455337674\n","[[3682   27   14]\n"," [  90   99   15]\n"," [  39    5   98]]\n","===============================================\n","average acc: 0.7044247514872605\n","average conf mat: [[3688.6   21.2   13.2]\n"," [ 107.8   80.2   16. ]\n"," [  47.6   13.2   81.2]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Mdtiox6Dc-l7","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1579192268621,"user_tz":-420,"elapsed":175301,"user":{"displayName":"Son Luu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOpA0EIHM56-K2ejgUcaTAXf9iBw3YjudaVluj=s64","userId":"07816337279734159476"}},"outputId":"5f80a034-2603-4c61-bffd-fcd97c1bf2b6"},"source":["# SVM + CNN \n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.pipeline import FeatureUnion\n","from joblib import dump\n","\n","# support function\n","def preprocess_deep(text):\n","    text = ViTokenizer.tokenize(text)\n","    text = unidecode(text)\n","    return text\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","#=========================== ENV ========================================\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/hatespeech_data.csv'\n","STOPWORDS = 'drive/My Drive/CODE/Hate speech detection/stopwords.txt'\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","#=========================== TRAIN DAT ==================================\n","data = pd.read_csv(DATA)\n","\n","O_X = data['free_text']\n","O_y = data['label_id']\n","\n","train_set = O_X\n","target_set = O_y\n","\n","#=========================== SVM feature extractor =====================\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        stop_words.append(line.strip('\\n'))\n","\n","BADWORDS = 'drive/My Drive/CODE/Hate speech detection/bad_words.txt'\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","bad_words_count = TfidfVectorizer(vocabulary=list(set(badwords)), lowercase=True)\n","\n","features_extractor = FeatureUnion([\n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count)\n","])\n","features_extractor.fit([preprocess(str(t)) for t in train_set])\n","\n","#============================== TextCNN feature extractor =================\n","tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n","tokenizer.fit_on_texts(train_set.astype(str))\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = min(max_features, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embed_size))\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","\n","#==================== Text CNN neural model =============================\n","filter_sizes = [1,2,3,5]\n","num_filters = 32\n","\n","inp = Input(shape=(maxlen,))\n","x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n","x = SpatialDropout1D(0.4)(x)\n","x = Reshape((maxlen, embed_size, 1))(x)\n","\n","conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[1], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[2], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","conv_3 = Conv2D(num_filters, kernel_size=(filter_sizes[3], embed_size), kernel_initializer='normal',\n","                activation='elu')(x)\n","\n","maxpool_0 = MaxPool2D(pool_size=(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n","maxpool_1 = MaxPool2D(pool_size=(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n","maxpool_2 = MaxPool2D(pool_size=(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n","maxpool_3 = MaxPool2D(pool_size=(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n","\n","z = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])\n","z = Flatten()(z)\n","z = Dropout(0.1)(z)\n","\n","\n","outp = Dense(3, activation=\"sigmoid\")(z)\n","\n","text_cnn_model = Model(inputs=inp, outputs=outp)\n","text_cnn_model.compile(loss='binary_crossentropy',\n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# ========================== SVM model =====================================\n","svm_model = svm.SVC(kernel='linear', C=1, probability=True)\n","\n","\n","# Kfold cross validation\n","results = []\n","confuses = []\n","kfold = StratifiedShuffleSplit(n_splits=5, test_size=0.2)\n","\n","count = 1\n","X = train_set\n","y = target_set\n","\n","for train, test in kfold.split(X, y):\n","  X_train_fold = X[train]\n","  y_train_fold = y[train]\n","\n","  X_test_fold = X[test]\n","  y_test_fold = y[test].values\n","  \n","  # svm\n","  svm_X_train_fold = [preprocess(str(p)) for p in X_train_fold]\n","  svm_X_train_fold = features_extractor.transform(svm_X_train_fold)\n","  \n","  svm_X_test_fold = [preprocess(str(p)) for p in X_test_fold]\n","  svm_X_test_fold = features_extractor.transform(svm_X_test_fold)\n","\n","  svm_model.fit(svm_X_train_fold, y_train_fold)\n","  svm_prediction = svm_model.predict_proba(svm_X_test_fold)\n","  svm_prediction_label = svm_model.predict(svm_X_test_fold)\n","\n","  # TextCNN \n","  cnn_X_train_fold = [preprocess_deep(str(p)) for p in list(X[train])]\n","  cnn_X_train_fold = tokenizer.texts_to_sequences(cnn_X_train_fold)\n","  cnn_X_train_fold = sequence.pad_sequences(cnn_X_train_fold, maxlen=maxlen)\n","\n","  cnn_X_test_fold = [preprocess_deep(str(p)) for p in list(X[test])]\n","  cnn_X_test_fold = tokenizer.texts_to_sequences(cnn_X_test_fold)\n","  cnn_X_test_fold = sequence.pad_sequences(cnn_X_test_fold, maxlen=maxlen)\n","\n","  cnn_y_train_fold = to_categorical(y_train_fold, num_classes=3)\n","  cnn_y_test_fold = cnn_y_train_fold\n","\n","  text_cnn_model.fit(cnn_X_train_fold, cnn_y_train_fold, batch_size=batch_size, epochs=epochs, verbose=1)\n","  cnn_prediction = text_cnn_model.predict(cnn_X_test_fold, batch_size=batch_size, verbose=1)\n","  cnn_prediction_label = cnn_prediction.argmax(axis=-1)\n","\n","  cnn_evaluate = f1_score(y_test_fold, cnn_prediction_label, average='macro')\n","  svm_evaluate = f1_score(y_test_fold, svm_prediction_label, average='macro')\n","\n","  print(cnn_evaluate, svm_evaluate)\n","  full_predict = []\n","  for i in range(0, len(svm_prediction)):\n","    k = np.mean(np.array([svm_prediction[i]*svm_evaluate] + [cnn_prediction[i]*cnn_evaluate]), axis=0)\n","    full_predict.append(k.argmax(axis=-1))\n","\n","#   for i in range(0, len(svm_prediction)):\n","#     if svm_prediction[i] == 0:\n","#       if cnn_prediction[i] == 1:\n","#         full_predict.append(cnn_prediction[i])\n","#       else:\n","#         full_predict.append(svm_prediction[i])\n","#     else:\n","#       full_predict.append(cnn_prediction[i])\n","\n","  evaluate = f1_score(y_test_fold, full_predict, average='macro')\n","  confuse = confusion_matrix(y_test_fold, full_predict, labels=[0, 1, 2])\n","\n","  print('===============================================')\n","  print(\"FOLD {}: {}\".format(count, evaluate))\n","  print(confuse)\n","  results.append(evaluate)\n","  confuses.append(confuse)\n","  print('===============================================')\n","\n","  count = count + 1\n","\n","print(\"average acc: {}\".format(str(np.mean(results))))\n","print(confuses)\n","print(\"average conf mat: {}\".format(np.mean(confuses, axis=-3)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Epoch 1/10\n","16276/16276 [==============================] - 20s 1ms/step - loss: 0.4030 - acc: 0.8334\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.2172 - acc: 0.9433\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.1902 - acc: 0.9436\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.1646 - acc: 0.9469\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.1406 - acc: 0.9526\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.1206 - acc: 0.9579\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.1082 - acc: 0.9612\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.0999 - acc: 0.9625\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0917 - acc: 0.9654\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.0848 - acc: 0.9680\n","4069/4069 [==============================] - 2s 390us/step\n","0.6617138433428232 0.6578158831230843\n","===============================================\n","FOLD 1: 0.664375754525667\n","[[3699   11   13]\n"," [ 116   69   19]\n"," [  56   18   68]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0854 - acc: 0.9682\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0805 - acc: 0.9695\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0751 - acc: 0.9717\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.0703 - acc: 0.9729\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0662 - acc: 0.9744\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0625 - acc: 0.9758\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0594 - acc: 0.9772\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0554 - acc: 0.9786\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0513 - acc: 0.9809\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0485 - acc: 0.9822\n","4069/4069 [==============================] - 1s 187us/step\n","0.7026724297405482 0.6388371364965847\n","===============================================\n","FOLD 2: 0.6610236677780791\n","[[3689   22   12]\n"," [ 121   69   14]\n"," [  51   23   68]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0574 - acc: 0.9792\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0542 - acc: 0.9805\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0499 - acc: 0.9823\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 571us/step - loss: 0.0466 - acc: 0.9833\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0430 - acc: 0.9850\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.0402 - acc: 0.9861\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0370 - acc: 0.9868\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0355 - acc: 0.9880\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0333 - acc: 0.9890\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0312 - acc: 0.9893\n","4069/4069 [==============================] - 1s 186us/step\n","0.8369881139781733 0.6916607006719065\n","===============================================\n","FOLD 3: 0.7876391115820739\n","[[3694   17   12]\n"," [  90  107    7]\n"," [  34   11   97]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0375 - acc: 0.9875\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0338 - acc: 0.9883\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.0304 - acc: 0.9895\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0287 - acc: 0.9907\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0262 - acc: 0.9912\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0255 - acc: 0.9916\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 572us/step - loss: 0.0227 - acc: 0.9932\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0223 - acc: 0.9931\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 572us/step - loss: 0.0208 - acc: 0.9930\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 571us/step - loss: 0.0189 - acc: 0.9939\n","4069/4069 [==============================] - 1s 188us/step\n","0.8835923672170972 0.6521729116696761\n","===============================================\n","FOLD 4: 0.8397605928513\n","[[3704   15    4]\n"," [  62  125   17]\n"," [  23    6  113]]\n","===============================================\n","Epoch 1/10\n","16276/16276 [==============================] - 9s 575us/step - loss: 0.0256 - acc: 0.9918\n","Epoch 2/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.0222 - acc: 0.9928\n","Epoch 3/10\n","16276/16276 [==============================] - 9s 573us/step - loss: 0.0202 - acc: 0.9936\n","Epoch 4/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0197 - acc: 0.9940\n","Epoch 5/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0183 - acc: 0.9944\n","Epoch 6/10\n","16276/16276 [==============================] - 9s 576us/step - loss: 0.0168 - acc: 0.9954\n","Epoch 7/10\n","16276/16276 [==============================] - 9s 571us/step - loss: 0.0151 - acc: 0.9959\n","Epoch 8/10\n","16276/16276 [==============================] - 9s 572us/step - loss: 0.0152 - acc: 0.9954\n","Epoch 9/10\n","16276/16276 [==============================] - 9s 574us/step - loss: 0.0142 - acc: 0.9961\n","Epoch 10/10\n","16276/16276 [==============================] - 9s 577us/step - loss: 0.0132 - acc: 0.9961\n","4069/4069 [==============================] - 1s 187us/step\n","0.9378954154522946 0.6425948257620152\n","===============================================\n","FOLD 5: 0.8961443870200784\n","[[3715    6    2]\n"," [  48  150    6]\n"," [  14    9  119]]\n","===============================================\n","average acc: 0.7697887027514396\n","[array([[3699,   11,   13],\n","       [ 116,   69,   19],\n","       [  56,   18,   68]]), array([[3689,   22,   12],\n","       [ 121,   69,   14],\n","       [  51,   23,   68]]), array([[3694,   17,   12],\n","       [  90,  107,    7],\n","       [  34,   11,   97]]), array([[3704,   15,    4],\n","       [  62,  125,   17],\n","       [  23,    6,  113]]), array([[3715,    6,    2],\n","       [  48,  150,    6],\n","       [  14,    9,  119]])]\n","average conf mat: [[3700.2   14.2    8.6]\n"," [  87.4  104.    12.6]\n"," [  35.6   13.4   93. ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"N0LI2buKiHgT","colab_type":"code","colab":{}},"source":["# example feature\n","\n","import pandas as pd\n","\n","# read data\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn import svm\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import f1_score, confusion_matrix\n","from sklearn.model_selection import train_test_split, cross_val_score\n","from sklearn.pipeline import FeatureUnion\n","import numpy as np\n","from unidecode import unidecode\n","from joblib import dump\n","from sklearn.model_selection import StratifiedShuffleSplit\n","from sklearn.preprocessing import FunctionTransformer\n","\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","\n","BADWORDS = 'drive/My Drive/HSD/bad_words.txt'\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","print(badwords)\n","\n","MODEL_FILE = 'drive/My Drive/HSD/MODEL/SVM.joblib'\n","STOPWORDS = 'drive/My Drive/HSD/stopwords.txt'\n","\n","original_data = pd.read_csv('drive/My Drive/HSD/data/hatespeech_data.csv')\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","# O_X = [preprocess(str(t)) for t in O_X]\n","\n","# features extraction\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        stop_words.append(line.strip('\\n'))\n","\n","\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","# bad_words_count = CountVectorizer(max_df=0.85,stop_words=badwords,max_features=10000)\n","bad_words_count = TfidfVectorizer(vocabulary=list(set(badwords)), lowercase=True)\n","len_of_text = FunctionTransformer(len)\n","\n","features_extractor = FeatureUnion([\n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count),\n","])\n","\n","# THEM VO FEATURE VE DO DAI CAU\n","\n","# Train and test division\n","train_set = O_X\n","target_set = O_y\n","\n","# OX_train, OX_test, Oy_train, Oy_test = train_test_split(train_set, target_set, test_size=0.1, random_state=42)\n","\n","training_data = [preprocess(str(t)) for t in train_set]\n","\n","features_extractor.fit(training_data)\n","len_of_text.fit(training_data)\n","\n","print(len_of_text.transform(training_data))\n","\n","transformed_text = features_extractor.transform(training_data)\n","# print((transformed_text.get_shape()))\n","# def get_new_coeff(p_text, badwords, transformed_text):\n","#     coeff = []\n","#     # p_text = [preprocess(str(t)) for t in text]\n","#     for t in p_text:\n","#         num_of_bad_word = 1\n","#         for w in t.split():\n","#             if w in badwords:\n","#                 num_of_bad_word = num_of_bad_word + 1\n","#         coeff.append(num_of_bad_word/len(t))\n","\n","#     for i in range(0, len(coeff)):\n","#         transformed_text[i] = transformed_text[i] * coeff[i]\n","#     # return np.array(coeff)\n","#     return transformed_text\n","\n","# coeff = get_new_coeff(train_set, badwords)\n","\n","\n","# print(transformed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kXy4ogRQBQjS","colab_type":"code","colab":{}},"source":["# merge 2 dataset\n","import pandas as pd\n","\n","a = pd.read_csv('drive/My Drive/CODE/Hate speech detection/raw_data/02_train_text.csv')\n","b = pd.read_csv('drive/My Drive/CODE/Hate speech detection/raw_data/03_train_label.csv')\n","\n","c = a.merge(b, on='id')\n","c.to_csv('drive/My Drive/CODE/Hate speech detection/dataset/hsd_data.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XER9eWfmaz4C","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1580222190018,"user_tz":-420,"elapsed":24794,"user":{"displayName":"Son Luu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOpA0EIHM56-K2ejgUcaTAXf9iBw3YjudaVluj=s64","userId":"07816337279734159476"}},"outputId":"f43fb15b-f9d9-4c74-b94a-0f650982b271"},"source":["# error analysis Traditional model\n","import numpy as np\n","import pandas as pd\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n","from sklearn.pipeline import FeatureUnion\n","from sklearn.externals import joblib\n","\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model_social/SVM/svm_model.h5'\n","\n","DATA = 'drive/My Drive/CODE/Hate speech detection/clean_dataset.csv'\n","\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(str(text))\n","    # text = unidecode(text)\n","    text = text.lower()\n","    return text\n","\n","STOPWORDS = 'drive/My Drive/CODE/Hate speech detection/stopwords.txt'\n","data = pd.read_csv('drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv')\n","\n","label0 = data.loc[data['label_id']==0]\n","label1 = data.loc[data['label_id']==1]\n","label2 = data.loc[data['label_id']==2]\n","\n","frames = [label0.head(300), label1.head(300), label2.head(100)]\n","original_data = pd.concat(frames)\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","BADWORDS = 'drive/My Drive/CODE/Hate speech detection/bad_words.txt'\n","with open(BADWORDS, \"r\") as ins:\n","    badwords = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        badwords.append(preprocess(dd))\n","\n","with open(STOPWORDS, \"r\") as ins:\n","    stop_words = []\n","    for line in ins:\n","        dd = line.strip('\\n')\n","        stop_words.append(dd)\n","\n","# features extraction\n","vect_word = TfidfVectorizer(max_features=20000, lowercase=True, analyzer='word',\n","                        stop_words=stop_words, ngram_range=(1, 3), dtype=np.float32)\n","\n","vect_char = TfidfVectorizer(max_features=40000, lowercase=True, analyzer='char',\n","                        stop_words=stop_words, ngram_range=(3, 6), dtype=np.float32)\n","\n","bad_words_count = TfidfVectorizer(vocabulary=list(set(badwords)), lowercase=True)\n","\n","features_extractor = FeatureUnion([                              \n","    (\"vect_word\", vect_word),\n","    (\"vect_char\", vect_char),\n","    (\"bad_words_count\", bad_words_count), \n","])\n","\n","training_data = [preprocess(t) for t in O_X]\n","features_extractor.fit([preprocess(t) for t in data['free_text']])\n","train_data = features_extractor.transform(training_data)\n","\n","loaded_model = joblib.load(MODEL_FILE)\n","O_yp=loaded_model.predict(train_data)\n","\n","original_data['new_label_id'] = O_yp\n","original_data.to_csv(\"drive/My Drive/CODE/Hate speech detection/result_data/result_tradional_model.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n","  warnings.warn(msg, category=FutureWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:520: UserWarning: The parameter 'stop_words' will not be used since 'analyzer' != 'word'\n","  warnings.warn(\"The parameter 'stop_words' will not be used\"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"hqP1ToL2SUEH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":778},"executionInfo":{"status":"ok","timestamp":1580222165203,"user_tz":-420,"elapsed":175196,"user":{"displayName":"Son Luu","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAOpA0EIHM56-K2ejgUcaTAXf9iBw3YjudaVluj=s64","userId":"07816337279734159476"}},"outputId":"af14eb93-eb15-44ad-d51b-6fd2655d6c81"},"source":["# error analysis Deep neural model\n","import numpy as np\n","import pandas as pd\n","from keras.layers import Dense, Input, Bidirectional, GRU, Reshape, Conv2D, MaxPool2D, Concatenate, Flatten, Dropout\n","from keras.layers import Embedding\n","from keras.preprocessing import text, sequence\n","from keras.layers import GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model\n","from keras import backend as K\n","from keras.utils import to_categorical\n","from sklearn.metrics import f1_score, confusion_matrix\n","\n","from pyvi.ViTokenizer import ViTokenizer\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n","from unidecode import unidecode\n","from keras.models import load_model\n","\n","# support function\n","def preprocess(text):\n","    text = ViTokenizer.tokenize(text)\n","    # text = unidecode(text)\n","    # text = text.lower()\n","    return text\n","\n","\n","EMBEDDING_FILE = 'drive/My Drive/CODE/Hate speech detection/embedding/cc.vi.300.vec'\n","MODEL_FILE = 'drive/My Drive/CODE/Hate speech detection/model_social/TextCNN/textcnn_model.h5'\n","\n","DATA = 'drive/My Drive/CODE/Hate speech detection/data/clean_dataset.csv'\n","\n","max_features = 11221\n","maxlen = 1000\n","embed_size = 300\n","batch_size = 1024\n","epochs = 10\n","\n","# read data\n","data = pd.read_csv(DATA)\n","\n","label0 = data.loc[data['label_id']==0]\n","label1 = data.loc[data['label_id']==1]\n","label2 = data.loc[data['label_id']==2]\n","\n","frames = [label0.head(300), label1.head(300), label2.head(200)]\n","original_data = pd.concat(frames)\n","\n","O_X = original_data['free_text']\n","O_y = original_data['label_id']\n","\n","\n","# --------------TRICH XUAT DAC TRUNG -------------------------\n","tokenizer = text.Tokenizer(num_words=max_features, lower=True)\n","tokenizer.fit_on_texts(data['free_text'].astype(str))\n","\n","# --------------END TRICH XUAT DAC TRUNG -------------------------\n","\n","embeddings_index = {}\n","with open(EMBEDDING_FILE, encoding='utf8') as f:\n","    for line in f:\n","        values = line.rstrip().rsplit(' ')\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","\n","word_index = tokenizer.word_index\n","num_words = min(max_features, len(word_index) + 1)\n","embedding_matrix = np.zeros((num_words, embed_size))\n","for word, i in word_index.items():\n","    if i >= max_features:\n","        continue\n","\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","\n","\n","train_x = tokenizer.texts_to_sequences(O_X)\n","train_x = sequence.pad_sequences(train_x, maxlen=maxlen)\n","train_y = to_categorical(O_y, num_classes=3)\n","\n","model = load_model(MODEL_FILE)\n","\n","pred = model.predict(train_x, batch_size=batch_size, verbose=1)\n","O_yp = pred.argmax(axis=-1)\n","\n","original_data['new_label_id'] = O_yp\n","original_data.to_csv(\"drive/My Drive/CODE/Hate speech detection/result_data/result_deep_model.csv\", index=False)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","800/800 [==============================] - 7s 9ms/step\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pTByN9HhvKmM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1593257883740,"user_tz":-420,"elapsed":22604,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"f8b94916-d7fd-477a-c2c9-143850d9a07b"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V0X24Tg03pyu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1590916237822,"user_tz":-420,"elapsed":6627,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"12e31c62-3079-4991-ba5c-0e8fb9547599"},"source":["pip install unidecode"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\r\u001b[K     |█▍                              | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |██▊                             | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |████▏                           | 30kB 4.0MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 40kB 4.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 71kB 4.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 81kB 4.5MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 92kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 245kB 4.8MB/s \n","\u001b[?25hInstalling collected packages: unidecode\n","Successfully installed unidecode-1.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MJjiSvwE3rgT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"ok","timestamp":1593257769968,"user_tz":-420,"elapsed":10789,"user":{"displayName":"Sơn Lưu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiuWJIB-Kj03NohF54bi2lsV0RWqxXQ0iZQjuym=s64","userId":"09824077883060402796"}},"outputId":"3b697e70-85c4-424f-d88d-ce4f5e44467e"},"source":["pip install pyvi"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pyvi\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/e1/0e5bc6b5e3327b9385d6e0f1b0a7c0404f28b74eb6db59a778515b30fd9c/pyvi-0.1-py2.py3-none-any.whl (8.5MB)\n","\u001b[K     |████████████████████████████████| 8.5MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from pyvi) (0.22.2.post1)\n","Collecting sklearn-crfsuite\n","  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.4.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (0.15.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->pyvi) (1.18.5)\n","Collecting python-crfsuite>=0.8.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n","\u001b[K     |████████████████████████████████| 747kB 42.1MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (1.12.0)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (0.8.7)\n","Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-crfsuite->pyvi) (4.41.1)\n","Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n","Successfully installed python-crfsuite-0.9.7 pyvi-0.1 sklearn-crfsuite-0.3.6\n"],"name":"stdout"}]}]}